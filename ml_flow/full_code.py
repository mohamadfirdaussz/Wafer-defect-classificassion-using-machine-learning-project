# # # -*- coding: utf-8 -*-
# # """wafer defect.ipynb
# #
# # Automatically generated by Colab.
# #
# # Original file is located at
# #     https://colab.research.google.com/drive/1qx49IFv_UjLt5-d4QxxpXFlPUPVjpnjJ
# # """
# #
# # # Show wafer bin map example from the internet
# #
# # from IPython.display import Image
# # from IPython.core.display import HTML
# # Image(url= "https://www.disco.co.jp/eg/introduction/img/what_pc.png", width=600)

# # Read data file
# import pandas as pd


# # Commented out IPython magic to ensure Python compatibility.
# # === LOAD AND INSPECT WAFER MAP DATA ===

# # Import libraries
# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np
# import random

# # Load wafer map dataset from a pickle file (binary format used to store Python objects)
# df = pd.read_pickle("C:/Users/user/Desktop/fyp/LSWMD.pkl/LSWMD.pkl")

# # Show basic information about the dataframe (column names, data types, non-null counts)
# df.info()

# # Show the first few rows of the dataset to understand the structure
# df.head()

# # === DATA CLEANING ===

# # Fix column name typo: 'trianTestLabel' → 'trainTestLabel'
# df.rename(columns={'trianTestLabel':'trainTestLabel'}, inplace=True)

# # Convert 'waferIndex' from float to integer for consistency
# df.waferIndex = df.waferIndex.astype(int)

# # Show the last few rows to verify changes
# df.tail()

# # === VISUALIZATION: Wafer Index Distribution ===

# # Count how many times each wafer index appears
# uni_Index = np.unique(df.waferIndex, return_counts=True)

# # Plot the distribution of wafer indices
# plt.bar(uni_Index[0], uni_Index[1], color='gold', align='center', alpha=0.5)
# plt.title("Wafer Index Distribution")
# plt.xlabel("Wafer Index")
# plt.ylabel("Frequency")
# plt.xlim(0, 26)
# plt.ylim(30000, 34000)
# plt.show()

# # Drop wafer index column since it's no longer needed
# df = df.drop(['waferIndex'], axis=1)

# # === ADD NEW COLUMN: Wafer Map Dimensions ===

# # Function to get dimensions of each waferMap (assumes each is a 2D numpy array)
# def find_dim(x):
#     dim0 = np.size(x, axis=0)
#     dim1 = np.size(x, axis=1)
#     return dim0, dim1

# # Apply the function to all wafer maps and create a new column
# df['waferMapDim'] = df.waferMap.apply(find_dim)

# # Show sample rows with new dimension column
# df.sample(5)

# # === INSPECT WAFER MAP SIZE DISTRIBUTION ===

# # Count frequency of each unique wafer map dimension
# uni_waferDim = np.unique(df.waferMapDim, return_counts=True)

# # Alternatively, check distribution as a percentage
# df['waferMapDim'].value_counts(normalize=True)

# # === MEMORY USAGE ANALYSIS ===

# # Check how much memory the DataFrame uses
# df.memory_usage(deep=True)

# # === CLEAN LABEL FORMATTING AND DUPLICATION HANDLING ===

# # Create a copy to work on cleaned labels
# df2 = df.copy()

# # Clean 'failureType': keep only the first inner string (e.g., [['Center']] → 'Center')
# df2.failureType = df2.failureType.apply(lambda x: x[0][0] if len(x) > 0 else float("NaN"))

# # Clean 'trainTestLabel': same process
# df2.trainTestLabel = df2.trainTestLabel.apply(lambda x: x[0][0] if len(x) > 0 else float("NaN"))

# # View sample after cleaning
# df2.sample()

# # Check memory usage after label simplification
# df2.memory_usage(deep=True)

# # === CONVERT LABELS TO CATEGORICAL TYPE ===

# # Use pandas 'category' data type to save memory and improve performance
# df2['trainTestLabel'] = df2['trainTestLabel'].astype('category')
# df2['failureType'] = df2['failureType'].astype('category')

# # Check memory usage again after conversion
# df2.memory_usage(deep=True)

# # === VISUALIZE FAILURE TYPES ===

# # Pie chart of failure type distribution (proportions)
# df2['failureType'].value_counts(normalize=True).plot.pie(
#     startangle=90, cmap="tab10", figsize=(8, 8), title="Failure Type Distribution"
# )
# plt.show()

# # Bar chart (optional, commented out)
# # df2['failureType'].value_counts().plot.bar()

# # Print failure type counts
# df2['failureType'].value_counts()

# # Show total number of labeled (non-null) images
# df2['failureType'].value_counts().sum()

# # === PREP FOR FILTERING ===

# # Before filtering, convert categories to strings to make conditional filtering easier
# df2['failureType'] = df2['failureType'].astype(str)
# df2['trainTestLabel'] = df2['trainTestLabel'].astype(str)

# # Mapping failureType and trainTestLabel to numerical values
# mapping_type = {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,
#                 'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}
# mapping_traintest = {'Training': 0, 'Test': 1}

# df2['failureNum'] = df2['failureType'].map(mapping_type)
# df2['trainTestNum'] = df2['trainTestLabel'].map(mapping_traintest)

# # Filtering out unlabeled data and 'Near-full' type
# df2 = df2[df2['failureNum'].between(0, 8)]
# df2 = df2[df2['failureType'] != 'Near-full']

# # Convert back to categorical and remove unused categories
# df2['failureType'] = df2['failureType'].astype('category')
# df2['failureType'] = df2['failureType'].cat.remove_unused_categories()

# # Filtering out wafer maps that are too small
# df2 = df2[df2['waferMapDim'].apply(lambda x: all(np.greater(x, (5,5))))]

# # Get the number of rows
# num_rows = df2.shape[0]
# print(num_rows)

# # Generate new dataset with balanced classes
# random.seed(10)  # Set seed for reproducibility
# failure_types = list(df2['failureType'].cat.categories)  # Get unique failure types
# num_cat = len(failure_types)

# df_list = []  # Use a list to store sampled DataFrames

# for i_cat in range(num_cat):
#     cat = failure_types[i_cat]
#     sample = df2[df2['failureType'] == cat].sample(n=500, replace=True, random_state=10)
#     df_list.append(sample)  # Append to list instead of using append()

# # Use pd.concat() to merge all sampled data
# df = pd.concat(df_list, ignore_index=True)

# # Verify class balance
# print(df['failureType'].value_counts())

# # Plot 5 of each failure type

# sample_size = 5
# fig, axs = plt.subplots(num_cat, sample_size, figsize = (15,15))
# for i_cat in range(num_cat):
#     cat = failure_types[i_cat]
#     random.seed(10)
#     sample = df2.loc[df2['failureType'] == cat].sample(sample_size)
#     for i in range(len(sample)):
#         index = sample.index[i]
#         axs[i_cat, i].axis('off')
#         axs[i_cat, i].imshow(sample['waferMap'][index])
#         axs[i_cat, i].set_title(f'{cat} - {index}')

# plt.tight_layout()
# plt.show()

# # Look at train/test split

# df['trainTestLabel'].value_counts(dropna = False, normalize = True)

# # loading libraries

# from scipy import ndimage

# import numpy as np
# import matplotlib.pyplot as plt
# from scipy import ndimage

# #  Drop NaNs and ensure 'failureType' is categorical
# df = df.dropna(subset=['failureType'])
# df['failureType'] = df['failureType'].astype('category')

# # Get valid categories
# failure_types = df['failureType'].cat.categories
# num_cat = len(failure_types)

# # Define sample size and figure layout
# sample_size = 3
# fig, axs = plt.subplots(num_cat, sample_size * 2, figsize=(30, 30))
# axs = np.atleast_2d(axs)  # Ensure axs is always 2D

# np.random.seed(10)  # Set seed for reproducibility

# #  Iterate through categories
# for i_cat, cat in enumerate(failure_types):
#     available_rows = df[df['failureType'] == cat]

#     # Use all available samples if less than required
#     sample = available_rows.sample(n=min(sample_size, len(available_rows)), random_state=10)

#     for i, (index, row) in enumerate(sample.iterrows()):
#         original = row['waferMap']
#         denoised = ndimage.median_filter(original, size=(2, 2))

#         #  Plot Original
#         axs[i_cat, i * 2].imshow(original)
#         axs[i_cat, i * 2].axis('off')
#         axs[i_cat, i * 2].set_title(f'Original \n{cat} - {index}')

#         #  Plot Denoised
#         axs[i_cat, i * 2 + 1].imshow(denoised, cmap='plasma')
#         axs[i_cat, i * 2 + 1].axis('off')
#         axs[i_cat, i * 2 + 1].set_title(f'De-noised \n{cat} - {index}')

# plt.tight_layout()
# plt.show()

# # Apply denoising to entire dataset

# import warnings
# warnings.filterwarnings('ignore')
# df3 = df.copy()
# for i in range(len(df3['waferMap'])):
#     original = df3['waferMap'].iloc[i]
#     df3['waferMap'].iloc[i] = ndimage.median_filter(original, size = (2,2))

# # Make the grid

# an = np.linspace(0, 2*np.pi, 100)
# plt.plot(2.5*np.cos(an), 2.5*np.sin(an))
# plt.axis('equal')
# plt.axis([-4, 4, -4, 4])
# for i in range(6):
#     plt.plot([-2.5, 2.5], [2.5 - i, 2.5 - i])
# for i in range(6):
#     plt.plot([2.5 - i, 2.5 - i], [-2.5, 2.5])

# # Number the regions

# for i in range(5):
#     plt.text(-2.3 + i, 1.8, 1, {'color': 'C0', 'fontsize': 13})
# for i in range(5):
#     plt.text(1.9, 1.8 - i, 2, {'color': 'C0', 'fontsize': 13})
# for i in range(5):
#     plt.text(-2.3 + i, -2.2, 3, {'color': 'C0', 'fontsize': 13})
# for i in range(5):
#     plt.text(-2.1, 1.8 - i, 4, {'color': 'C0', 'fontsize': 13})
# for row in range(3):
#     for i in range(3):
#         plt.text(-1.3 + i, 0.8 - row, i + 5 + row * 3, {'color': 'C0', 'fontsize': 13})

# plt.title(" Devide wafer map into 13 regions")
# plt.xticks([])
# plt.yticks([])
# plt.show()

# # Define functions to calculate densities of the regions

# def cal_den(x):
#     return 100 * (np.sum(x==2)/np.size(x))

# def find_regions(x):
#     rows=np.size(x,axis=0)
#     cols=np.size(x,axis=1)
#     #print(cols)
#     ind1=np.arange(0,rows,rows//5)
#     ind2=np.arange(0,cols,cols//5)
#     reg1=x[ind1[0]:ind1[1],:]
#     reg2=x[:,ind2[4]:]
#     reg3=x[ind1[4]:,:]
#     reg4=x[:,ind2[0]:ind2[1]]
#     reg5=x[ind1[1]:ind1[2],ind2[1]:ind2[2]]
#     reg6=x[ind1[1]:ind1[2],ind2[2]:ind2[3]]
#     reg7=x[ind1[1]:ind1[2],ind2[3]:ind2[4]]
#     reg8=x[ind1[2]:ind1[3],ind2[1]:ind2[2]]
#     reg9=x[ind1[2]:ind1[3],ind2[2]:ind2[3]]
#     reg10=x[ind1[2]:ind1[3],ind2[3]:ind2[4]]
#     reg11=x[ind1[3]:ind1[4],ind2[1]:ind2[2]]
#     reg12=x[ind1[3]:ind1[4],ind2[2]:ind2[3]]
#     reg13=x[ind1[3]:ind1[4],ind2[3]:ind2[4]]

#     fea_reg_den = []
#     fea_reg_den = [cal_den(reg1),cal_den(reg2),cal_den(reg3),cal_den(reg4),cal_den(reg5),cal_den(reg6),cal_den(reg7),cal_den(reg8),cal_den(reg9),cal_den(reg10),cal_den(reg11),cal_den(reg12),cal_den(reg13)]
#     return fea_reg_den

# # Apply the density calculation function

# df3['fea_reg']=df3.waferMap.apply(find_regions)

# import numpy as np
# import matplotlib.pyplot as plt

# # Ensure 'failureType' is categorical and drop NaNs
# df3 = df3.dropna(subset=['failureType'])
# df3['failureType'] = df3['failureType'].astype('category')

# # Get unique failure types
# failure_types = df3['failureType'].cat.categories
# num_cat = len(failure_types)

# # Set max number of plots to avoid empty subplots
# num_plots = min(num_cat, 8)  # Limit to 8 plots
# fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(30, 10))
# axs = axs.flatten()  # Convert to 1D array

# sample_size = 1  # Adjust as needed
# valid_plot_count = 0  # Track the number of valid plots

# #  Iterate over failure types (limit to available subplots)
# for i, cat in enumerate(failure_types):
#     if valid_plot_count >= num_plots:
#         break  # Stop if we've filled the available subplots

#     category_df = df3[df3['failureType'] == cat]

#     #  Skip if no samples exist for this category
#     if category_df.empty:
#         print(f"Warning: No samples available for category '{cat}'. Skipping.")
#         continue

#     # Ensure sample size does not exceed available rows
#     sample_size_actual = min(sample_size, len(category_df))
#     sample = category_df.sample(sample_size_actual, random_state=10)

#     # Check if 'fea_reg' column exists and has valid data
#     if 'fea_reg' not in sample.columns or sample['fea_reg'].isnull().values.any():
#         print(f"Warning: 'fea_reg' column missing or contains NaN for category '{cat}'. Skipping.")
#         continue

#     # Plot bar chart
#     feature_values = list(sample['fea_reg'])[0]
#     axs[valid_plot_count].bar(np.linspace(1, 13, 13), feature_values)
#     axs[valid_plot_count].set_title(cat, fontsize=15)

#     # Set y-axis limit dynamically
#     y_lim = max(feature_values)
#     axs[valid_plot_count].fill_betweenx((0, y_lim), 0, 4.5, alpha=0.5, color='g')
#     axs[valid_plot_count].fill_betweenx((0, y_lim), 4.5, 8.5, alpha=0.5, color='y')
#     axs[valid_plot_count].fill_betweenx((0, y_lim), 8.5, 9.5, alpha=0.5, color='r')
#     axs[valid_plot_count].fill_betweenx((0, y_lim), 9.5, 13.5, alpha=0.5, color='y')

#     # Label sections
#     axs[valid_plot_count].text(2, y_lim, 'Edges', {'color': 'k', 'fontsize': 12})
#     axs[valid_plot_count].text(4.5, y_lim * 0.9, 'Around the center', {'color': 'k', 'fontsize': 12})
#     axs[valid_plot_count].text(8.5, y_lim, 'Center', {'color': 'k', 'fontsize': 12})
#     axs[valid_plot_count].text(9.5, y_lim * 0.9, 'Around the center', {'color': 'k', 'fontsize': 12})

#     valid_plot_count += 1  # Increment valid plot count

# # Hide unused subplots
# for i in range(valid_plot_count, len(axs)):
#     fig.delaxes(axs[i])

# plt.show()

# # Change the 1s to 0s so that the wafer maps only contain two types of signals: faulty and not faulty

# def change_val(img):
#     img[img==1] = 0
#     return img

# df3['new_waferMap'] = df3.waferMap.apply(change_val)

# import numpy as np
# import matplotlib.pyplot as plt
# from skimage.transform import radon

# # Ensure 'failureType' is categorical and drop NaNs
# df3 = df3.dropna(subset=['failureType', 'waferMap'])
# df3['failureType'] = df3['failureType'].astype('category')

# # Get unique failure types
# failure_types = df3['failureType'].cat.categories
# num_cat = len(failure_types)

# # Dynamically set subplot grid size based on available categories
# rows = (num_cat // 4) + (num_cat % 4 > 0)  # Max 4 columns per row
# cols = min(num_cat, 4)  # Max 4 columns
# fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, rows * 5))
# ax = ax.flatten()  # Convert to 1D array

# sample_size = 1  # Adjust as needed
# valid_plot_count = 0  # Track the number of valid plots

# # Iterate over failure types (limit to available subplots)
# for i, cat in enumerate(failure_types):
#     category_df = df3[df3['failureType'] == cat]  # Corrected DataFrame reference

#     # Skip if no samples exist for this category
#     if category_df.empty:
#         print(f"Warning: No samples available for category '{cat}'. Skipping.")
#         continue

#     # Ensure sample size does not exceed available rows
#     sample_size_actual = min(sample_size, len(category_df))
#     sample = category_df.sample(sample_size_actual, random_state=10)

# import numpy as np
# import pandas as pd
# from skimage import measure
# from scipy import stats

# # Get image
# img = list(sample['waferMap'])[0]

# # Ensure image is valid before applying transformation
# if img is None or not isinstance(img, np.ndarray):
#     print(f"Warning: Invalid image for category '{cat}'. Skipping.")


# # Compute Radon transform
# theta = np.linspace(0., 180., max(img.shape), endpoint=False)
# sinogram = radon(img, theta=theta)

# # Display transformed image
# ax[valid_plot_count].imshow(sinogram, cmap=plt.cm.Greys_r, extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')
# ax[valid_plot_count].set_title(cat, fontsize=15)
# ax[valid_plot_count].set_xticks([])

# valid_plot_count += 1  # Increment valid plot count

# # Hide unused subplots
# for i in range(valid_plot_count, len(ax)):
#     fig.delaxes(ax[i])

# plt.tight_layout()
# plt.show()

# # Define functions for cubis interpolation

# from scipy import interpolate

# def cubic_inter_mean(img):
#     theta = np.linspace(0., 180., max(img.shape), endpoint=False)
#     sinogram = radon(img, theta=theta)
#     xMean_Row = np.mean(sinogram, axis = 1)
#     x = np.linspace(1, xMean_Row.size, xMean_Row.size)
#     y = xMean_Row
#     f = interpolate.interp1d(x, y, kind = 'cubic')
#     xnew = np.linspace(1, xMean_Row.size, 20)
#     ynew = f(xnew)/100   # use interpolation function returned by `interp1d`
#     return ynew

# def cubic_inter_std(img):
#     theta = np.linspace(0., 180., max(img.shape), endpoint=False)
#     sinogram = radon(img, theta=theta)
#     xStd_Row = np.std(sinogram, axis=1)
#     x = np.linspace(1, xStd_Row.size, xStd_Row.size)
#     y = xStd_Row
#     f = interpolate.interp1d(x, y, kind = 'cubic')
#     xnew = np.linspace(1, xStd_Row.size, 20)
#     ynew = f(xnew)/100   # use interpolation function returned by `interp1d`
#     return ynew

# # Apply cubic interpolation

# df3['fea_cub_mean'] = df3.waferMap.apply(cubic_inter_mean)
# df3['fea_cub_std'] = df3.waferMap.apply(cubic_inter_std)

# import numpy as np
# import matplotlib.pyplot as plt

# # Remove NaN values from relevant columns
# df3 = df3.dropna(subset=['failureType', 'fea_cub_mean', 'fea_cub_std'])

# # Ensure 'failureType' is categorical
# df3['failureType'] = df3['failureType'].astype('category')

# # Get unique failure types
# failure_types = df3['failureType'].cat.categories
# num_cat = len(failure_types)

# # Check if there are any categories to plot
# if num_cat == 0:
#     print("No valid failure categories found. Exiting plot generation.")
# else:
#     # Dynamically adjust grid size (max 4 columns)
#     total_subplots = num_cat * 2  # Each category needs 2 subplots (mean + std dev)
#     rows = (total_subplots // 4) + (total_subplots % 4 > 0)
#     cols = min(total_subplots, 4)

#     # Increase figure size for better visibility
#     fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 25))
#     ax = ax.flatten()  # Flatten the axis array for easy indexing

#     plot_count = 0  # Tracks valid plots

#     # Loop through failure types
#     for cat in failure_types:
#         # Filter for the category and check if it has valid data
#         category_df = df3[df3['failureType'] == cat]
#         if category_df.empty:
#             print(f" Warning: No samples available for category '{cat}'. Skipping.")
#             continue

#         # Ensure enough samples exist
#         sample_size_actual = min(len(category_df), 1)  # Prevents sampling error
#         sample = category_df.sample(sample_size_actual, random_state=10)

#         # Get features and check if they're valid
#         fea_mean = sample['fea_cub_mean'].values[0]
#         fea_std = sample['fea_cub_std'].values[0]

#         if len(fea_mean) == 0 or len(fea_std) == 0:
#             print(f"Warning: Invalid feature data for category '{cat}'. Skipping.")
#             continue

#         # Dynamically adjust y-axis limit based on data
#         y_max = max(max(fea_mean), max(fea_std)) * 1.2  # Increase by 20% for visibility

#         # Ensure plot count is within range before plotting
#         if plot_count >= len(ax):
#             print(f"Skipping additional plots due to subplot limit.")
#             break

#         # Plot Row Mean with larger bars
#         ax[plot_count].bar(np.linspace(1, 20, 20), fea_mean, color='blue', width=0.5)
#         ax[plot_count].set_title(f'{cat} - Mean', fontsize=15)
#         ax[plot_count].set_xticks(range(1, 21, 2))
#         ax[plot_count].set_xlim([0, 21])
#         ax[plot_count].set_ylim([0, y_max])
#         ax[plot_count].grid(True, linestyle='--', alpha=0.7)  # Added grid for better visibility
#         plot_count += 1  # Move to the next subplot

#         # Ensure plot count is within range before plotting the second subplot
#         if plot_count >= len(ax):
#             print(f"Skipping additional plots due to subplot limit.")
#             break

#         # Plot Row Standard Deviation with larger bars
#         ax[plot_count].bar(np.linspace(1, 20, 20), fea_std, color='red', width=0.5)
#         ax[plot_count].set_title(f'{cat} - Std Dev', fontsize=15)
#         ax[plot_count].set_xticks(range(1, 21, 2))
#         ax[plot_count].set_xlim([0, 21])
#         ax[plot_count].set_ylim([0, y_max])
#         ax[plot_count].grid(True, linestyle='--', alpha=0.7)  # ✅ Added grid for better visibility
#         plot_count += 1  # Move to the next subplot

#     # Remove any unused subplots (if fewer categories than available subplots)
#     for i in range(plot_count, len(ax)):
#         fig.delaxes(ax[i])

#     plt.tight_layout()
#     plt.show()

# import numpy as np
# import matplotlib.pyplot as plt
# from skimage import measure
# from scipy import stats

# # Ensure 'failureType' is categorical
# df3['failureType'] = df3['failureType'].astype('category')

# # Get valid failure categories (exclude empty ones)
# failure_types = [cat for cat in df3['failureType'].cat.categories if not df3[df3['failureType'] == cat].empty]
# num_cat = len(failure_types)

# # Exit early if no valid categories exist
# if num_cat == 0:
#     print("No valid failure categories found. Exiting plot generation.")
# else:
#     # Dynamically adjust rows & cols based on num_cat
#     cols = min(num_cat, 4)  # Max 4 columns
#     rows = -(-num_cat // cols)  # Ceiling division to get required rows

#     # Create figure with exact required subplots
#     fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(5 * cols, 5 * rows))
#     ax = np.array(ax).flatten()  # Flatten for easier indexing

#     plot_count = 0  # Track the number of valid plots

#     for cat in failure_types:
#         # Filter data and ensure we have samples
#         category_df = df3[df3['failureType'] == cat]
#         if category_df.empty:
#             continue  # Skip empty categories

#         # Sample 1 valid entry
#         sample_size_actual = min(len(category_df), 1)  # Prevents sampling errors
#         sample = category_df.sample(sample_size_actual, random_state=10)

#         #  Extract wafer map
#         img = sample['waferMap'].values[0]
#         zero_img = np.zeros(img.shape)

#         #  Label connected components
#         img_labels = measure.label(img, connectivity=1) - 1  # Shift to 0-based index

#         # Identify most frequent region (excluding background)
#         unique_labels = img_labels[img_labels > -1]
#         no_region = 0 if unique_labels.size == 0 else stats.mode(unique_labels, axis=None, keepdims=True).mode.item()

#         # Highlight salient region
#         zero_img[np.where(img_labels == no_region)] = 2

#         # Plot
#         ax[plot_count].imshow(zero_img, cmap='jet')  # Better visibility
#         ax[plot_count].set_title(cat, fontsize=12)
#         ax[plot_count].set_xticks([])
#         ax[plot_count].set_yticks([])
#         plot_count += 1  # Increase count of plotted graphs

#     #  Remove unused subplots
#     for j in range(plot_count, len(ax)):
#         fig.delaxes(ax[j])

#     plt.tight_layout()
#     plt.show()

# # Fix measure.label() issue (remove 'neighbors' argument)
# def fea_geom(img):
#     # Ensure img is a valid array
#     if img is None or not isinstance(img, np.ndarray):
#         return None

#     img_labels = measure.label(img, connectivity=1, background=0)  # Fixed

#     if img_labels.max() == 0:
#         img_labels[img_labels == 0] = 1  # Assign dummy label

#     props = measure.regionprops(img_labels)

#     if not props:
#         return None  # No regions detected

#     # Extract properties
#     prop_area = props[0].area
#     prop_perimeter = props[0].perimeter
#     prop_majaxis = props[0].major_axis_length
#     prop_minaxis = props[0].minor_axis_length
#     prop_ecc = props[0].eccentricity
#     prop_solidity = props[0].solidity

#     return (prop_area, prop_perimeter, prop_majaxis, prop_minaxis, prop_ecc, prop_solidity)

# # Ensure 'waferMap' has no NaN before applying fea_geom
# df3 = df3.dropna(subset=['waferMap'])

# # Apply function safely
# df3['fea_geom'] = df3['waferMap'].apply(fea_geom)

# # Combine the features

# df_all=df3.copy()
# a=[df_all.fea_reg.iloc[i] for i in range(df_all.shape[0])] #13
# b=[df_all.fea_cub_mean.iloc[i] for i in range(df_all.shape[0])] #20
# c=[df_all.fea_cub_std.iloc[i] for i in range(df_all.shape[0])] #20
# d=[df_all.fea_geom.iloc[i] for i in range(df_all.shape[0])] #6
# fea_all = np.concatenate((np.array(a),np.array(b),np.array(c),np.array(d)),axis=1) #59 in total

# # Create target column array

# label=[df_all.failureNum.iloc[i] for i in range(df_all.shape[0])]
# label=np.array(label)

# # Import necessary libraries
# from sklearn.model_selection import train_test_split
# from tensorflow.keras.utils import to_categorical  # Replacing np_utils
# from collections import Counter
# import numpy as np

# # Assuming fea_all contains feature data and label contains class labels
# X = fea_all
# y = label

# # Split dataset into training and testing sets with stratification
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# # Print class distribution to ensure stratification worked
# print('Training target statistics:', Counter(y_train))
# print('Testing target statistics:', Counter(y_test))

# # Convert labels to one-hot encoding for neural networks (if required)
# y_train = to_categorical(y_train)
# y_test = to_categorical(y_test)

# # Set a random seed for reproducibility
# RANDOM_STATE = 42

# # Import necessary libraries
# from sklearn.svm import LinearSVC
# from sklearn.multiclass import OneVsOneClassifier
# from sklearn.model_selection import train_test_split, cross_val_score
# from sklearn.preprocessing import LabelEncoder
# import numpy as np

# # Assuming fea_all contains feature data and label contains class labels
# X = fea_all
# y = label

# # Convert labels to integer values if not already done
# if len(y.shape) > 1:
#     y = np.argmax(y, axis=1)  # Convert one-hot to class labels

# # Split dataset into training and testing sets with stratification
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# # Print class distribution
# from collections import Counter
# print('Training target statistics:', Counter(y_train))
# print('Testing target statistics:', Counter(y_test))

# # Initialize and train OneVsOne SVM
# RANDOM_STATE = 42
# SVM = OneVsOneClassifier(LinearSVC(random_state=RANDOM_STATE))
# SVM.fit(X_train, y_train)

# # Predictions
# y_train_pred = SVM.predict(X_train)
# y_test_pred = SVM.predict(X_test)

# # Cross-validation (on training set, not test set)
# scores = cross_val_score(SVM, X_train, y_train, cv=10, scoring='accuracy')

# # Print results
# print(f"Cross-validation Accuracy for Support Vector Machine: {scores.mean():.4f}")

# # Define function to present results

# import itertools
# from sklearn.metrics import confusion_matrix

# def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
#     """
#     This function prints and plots the confusion matrix.
#     Normalization can be applied by setting `normalize=True`.
#     """
#     if normalize:
#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
#         print("Normalized confusion matrix")
#     else:
#         print('Confusion matrix, without normalization')

#     #print(cm)

#     plt.imshow(cm, interpolation='nearest', cmap=cmap)
#     plt.title(title)
#     plt.colorbar()

#     fmt = '.2f' if normalize else 'd'
#     thresh = cm.max() / 2.
#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
#         plt.text(j, i, format(cm[i, j], fmt),
#                  horizontalalignment="center",
#                  color="white" if cm[i, j] > thresh else "black")

#     plt.tight_layout()
#     plt.ylabel('True label')
#     plt.xlabel('Predicted label')
#     labels = failure_types
#     plt.xticks(ticks = range(8), labels = labels)
#     plt.yticks(ticks = range(8), labels = labels)

# # Compute confusion matrix

# cnf_matrix = confusion_matrix(y_test, y_test_pred)
# np.set_printoptions(precision=2)

# from matplotlib import gridspec
# fig = plt.figure(figsize=(15, 8))
# gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

# ## Plot non-normalized confusion matrix

# plt.subplot(gs[0])
# plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# # Plot normalized confusion matrix

# plt.subplot(gs[1])
# plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

# plt.show()

# # Implement logistic regression

# from sklearn.linear_model import LogisticRegression

# LR = LogisticRegression().fit(X_train, y_train)
# y_train_pred = LR.predict(X_train)
# y_test_pred = LR.predict(X_test)
# scores = cross_val_score(LR, X_test, y_test, cv=10, scoring='accuracy')
# print(f"Accuracy score for the logistic regression: \n {scores.mean()}")

# # Compute confusion matrix

# cnf_matrix = confusion_matrix(y_test, y_test_pred)
# np.set_printoptions(precision=2)

# from matplotlib import gridspec
# fig = plt.figure(figsize=(15, 8))
# gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

# ## Plot non-normalized confusion matrix

# plt.subplot(gs[0])
# plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# # Plot normalized confusion matrix

# plt.subplot(gs[1])
# plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

# plt.show()

# # Implement random forest classifier

# from sklearn.ensemble import RandomForestClassifier

# RF = RandomForestClassifier().fit(X_train, y_train)
# y_train_pred = RF.predict(X_train)
# y_test_pred = RF.predict(X_test)
# scores = cross_val_score(RF, X_test, y_test, cv=10, scoring='accuracy')
# print(f"Accuracy score for the random forest classifier: \n {scores.mean()}")

# # Compute confusion matrix

# cnf_matrix = confusion_matrix(y_test, y_test_pred)
# np.set_printoptions(precision=2)

# from matplotlib import gridspec
# fig = plt.figure(figsize=(15, 8))
# gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

# ## Plot non-normalized confusion matrix

# plt.subplot(gs[0])
# plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# # Plot normalized confusion matrix

# plt.subplot(gs[1])
# plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

# plt.show()

# # Look at parameters used by our current forest

# from pprint import pprint
# print('Parameters currently in use:\n')
# pprint(RF.get_params())

# # Hyperparemeer tuning using grid search

# import random
# import numpy as np
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV

# # Set random seed for reproducibility
# random.seed(10)
# np.random.seed(10)

# # Define the hyperparameters grid
# n_estimators = [100, 300, 500]
# max_depth = [30, 40, 50]
# min_samples_split = [2, 5, 10]
# min_samples_leaf = [1, 2, 5]

# hyperF = dict(n_estimators=n_estimators,
#               max_depth=max_depth,
#               min_samples_split=min_samples_split,
#               min_samples_leaf=min_samples_leaf)

# # Define the Random Forest model
# RF = RandomForestClassifier(random_state=10)

# # Perform Grid Search
# gridF = GridSearchCV(RF, hyperF, cv=3, verbose=1, n_jobs=-1)
# bestF = gridF.fit(X_train, y_train)  # Ensure X_train and y_train are properly defined

# # Display best parameters
# print("Best Hyperparameters:", gridF.best_params_)

# # Look at the best parameters
# best = gridF.best_params_
# print(best)

# # Implement random forest classifier with new parameters
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import cross_val_score

# # Define the Random Forest model with best hyperparameters
# RF = RandomForestClassifier(
#     max_depth=best['max_depth'],
#     min_samples_leaf=best['min_samples_leaf'],
#     min_samples_split=best['min_samples_split'],
#     n_estimators=best['n_estimators'],
#     random_state=10
# )

# # Train the model
# RF.fit(X_train, y_train)

# # Make predictions
# y_train_pred = RF.predict(X_train)
# y_test_pred = RF.predict(X_test)

# # Evaluate the model using cross-validation
# scores = cross_val_score(RF, X_test, y_test, cv=10, scoring='accuracy')
# print(f"Accuracy score for Random Forest Classifier: {scores.mean():.4f}")

# # Compute confusion matrix
# cnf_matrix = confusion_matrix(y_test, y_test_pred)
# np.set_printoptions(precision=2)

# from matplotlib import gridspec
# fig = plt.figure(figsize=(15, 8))
# gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

# ## Plot non-normalized confusion matrix
# plt.subplot(gs[0])
# plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# # Plot normalized confusion matrix
# plt.subplot(gs[1])
# plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

# plt.show()

