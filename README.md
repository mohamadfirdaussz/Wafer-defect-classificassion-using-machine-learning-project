# SEMICONDUCTOR WAFER DEFECT CLASSIFICATION PIPELINE

A modular end-to-end pipeline for **wafer defect classification** using traditional machine learning techniques.  
The system performs **data ingestion**, **preprocessing**, **feature selection**, and **model training** with multiple classifiers and tuning strategies.

---
# üß† SEMICONDUCTOR WAFER DEFECT CLASSIFICATION ‚Äî DATA PREPROCESSING & FEATURE ENGINEERING

This repository contains the **first two stages** of the wafer defect classification pipeline:
1. **Data Loading & Preprocessing** (`data_loader.py`)
2. **Feature Engineering** (`feature_engineer.py`)
3. .......
4. .......
5. ......to be continued






## üìÅ PIPELINE OVERVIEW

### 1Ô∏è‚É£ DATA INGESTION & PRE-PROCESSING
- Load wafer map datasets (e.g., **WM-811K**, or cleaned CSVs)
- Handle missing/corrupted samples
- Normalize and reshape wafer map matrices
- Noise filtering and invalid pattern removal
- Save preprocessed data to structured CSV format

---

### 2Ô∏è‚É£ FEATURE EXTRACTION
- Generate region-based, geometric, and Radon-transform features
- Compute statistical and spatial metrics per wafer
- Label encoding for categorical attributes
- Output feature matrix for downstream ML models

---

### 3Ô∏è‚É£ FEATURE SELECTION
**Module:** `feature_selector.py`  
Selects the most relevant features using three tracks:

#### 4A ‚Äî Baseline
Use all numeric features without reduction.  
‚û°Ô∏è Output: `features_4A_baseline.csv`

#### 4B ‚Äî Filter/Wrapper
- Filter: ANOVA F-test (`SelectKBest`)
- Wrapper: Recursive Feature Elimination (RFE) using Random Forest  
  ‚û°Ô∏è Output: `features_4B_filter_wrapper.csv`

#### 4C ‚Äî Embedded
- **Lasso (Œ±=0.01)** for sparse linear selection
- **Random Forest** for tree-based importance
- Combines both selections for robust embedded feature extraction  
  ‚û°Ô∏è Output: `features_4C_embedded.csv`

> ‚öôÔ∏è LassoCV (cross-validated) replaced with single-run **Lasso(alpha=0.01)** for faster execution.  
> Suitable for large wafer datasets (10k+ samples).

---

### 4Ô∏è‚É£ MODEL TRAINING & HYPERPARAMETER TUNING
**Classifiers:**
- Support Vector Machine (SVM)
- Random Forest
- XGBoost
- Gradient Boosting (GBM)
- K-Nearest Neighbors (KNN)
- Logistic Regression
- Decision Tree

**Techniques:**
- Stratified K-Fold cross-validation
- GridSearchCV or RandomizedSearchCV for tuning key hyperparameters
- Evaluation metrics: Accuracy, F1-score, Confusion Matrix

---

### 5Ô∏è‚É£ RESULT STORAGE & ANALYSIS
- Saves selected features and trained model metrics
- CSV outputs for each selection track
- Final report includes:
    - Best performing model
    - Selected feature importance rankings
    - Performance comparison across classifiers

---

## ‚öôÔ∏è DIRECTORY STRUCTURE





###üìò NOTES

Embedded selection may take longer due to model fitting.BLUM SIAPPP
