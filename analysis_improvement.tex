\chapter{RESULTS AND DISCUSSION}

% -------------------------------------------------------------------------
% SECTION 4.1: OVERVIEW
% -------------------------------------------------------------------------
\section{Overview}
Building upon the methodology established in the preceding chapter, this section presents the empirical findings from the experimental evaluation of the proposed machine learning pipeline. The study systematically addresses the challenge of semiconductor wafer defect classification using the WM-811K dataset, focusing on three core research objectives: identifying an optimal feature engineering strategy, evaluating algorithmic performance across diverse mathematical paradigms, and assessing model generalizability for production environments.

A critical pillar of this analysis is the "Leak-Proof Gatekeeper" methodology implemented during Stage 3. To ensure scientific validity and simulate real-world deployment, the dataset was partitioned into a Training Set (70\%) and a Locked Test Set (30\%) immediately following data loading. Crucially, all iterative preprocessing—including feature expansion, hybrid class balancing (SMOTE and Undersampling), and feature selection—was derived exclusively from the training distribution. Consequently, the performance metrics reported on the Test Set reflect true generalization capability on organic, unseen data, mitigating common pitfalls like data leakage bias that frequently inflate accuracy in related literature.

The chapter follows a logical progression: Section \ref{sec:feature_analysis} evaluates the efficacy of high-dimensional feature selection; Section \ref{sec:results_overview} provides a comparative leaderboard of classification models; Section \ref{sec:algorithm_analysis} offers a theoretical examination of why linear models emerged superior; and Section \ref{sec:error_analysis} provides a granular inspection of misclassification patterns through the lens of semiconductor topology.

% -------------------------------------------------------------------------
% SECTION 4.2: FEATURE SELECTION ANALYSIS (OBJECTIVE 1)
% -------------------------------------------------------------------------
\section{Feature Selection Analysis (Objective 1)}
\label{sec:feature_analysis}

To address the first research objective, the study investigated various dimensionality reduction strategies. During Stage 3.5 (Feature Expansion), the original 66 base features were transformed into a high-dimensional feature space containing over 8,700 interaction terms via polynomial and cross-product mapping. Training models directly on this raw expanded set would induce the "Curse of Dimensionality," leading to catastrophic overfitting and prohibitive computational latency. To mitigate this, three distinct experimental "Tracks" were established to compare Wrapper and Embedded selection paradigms.

\subsection{Comparative Analysis of Selection Tracks}

The first track, Track 4B, utilized Recursive Feature Elimination (RFE) as a wrapper method. Treating selection as an iterative optimization problem, RFE systematically pruned the least significant features based on a base estimator’s weights until exactly 25 features remained. The objective was to isolate a "Golden Subset" that captures the non-linear interaction effects between geometric and intensity-based descriptors.

The second track, Track 4C, employed an embedded method based on Random Forest Gini Importance. This approach prioritized features that provided the highest information gain across an ensemble of decision trees. This track tested whether a tree-based, non-linear ranking would favor different descriptors—such as local density variations—over the global transform features.

The third track, Track 4D, implemented Lasso (Least Absolute Shrinkage and Selection Operator) Regularization. Lasso applies a sparsity-driven L1 penalty that forces irrelevant feature coefficients to zero. Unlike the fixed-size approaches in Tracks 4B and 4C, Lasso retained 39 non-zero features. This track aimed to determine if a mathematical sparsity constraint could select a broader, more robust set of features compared to iterative pruning.

\begin{table}[h!]
    \centering
    \caption{Comparative analysis of feature selection strategies based on the Macro F1-Score. Track 4B (RFE) yielded the highest performance with the most compact feature set.}
    \label{tab:feature_tracks}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l c c} 
    \toprule
    \textbf{Track ID} & \textbf{Selection Method} & \textbf{Feature Count} & \textbf{Best Model Macro F1-Score} \\
    \midrule
    \textbf{Track 4B} & \textbf{Wrapper (RFE)} & \textbf{25} & \textbf{0.617} \\ 
    Track 4D & Embedded (Lasso) & 39 & 0.584 \\ 
    Track 4C & Embedded (Random Forest) & 25 & 0.572 \\ 
    \bottomrule
    \end{tabular}
\end{table}

As summarized in Table \ref{tab:feature_tracks}, Track 4B (RFE) emerged as the superior strategy. Despite using fewer features than Lasso, RFE achieved a higher Macro F1-Score (0.617). This indicates that the iterative pruning of RFE was more effective at preserving the synergistic relationships between features, whereas Lasso's L1 penalty may have arbitrarily discarded highly correlated features that were essential for distinguishing subtle defects like 'Scratch' and 'Random'.

% -------------------------------------------------------------------------
% SECTION 4.3: EXPERIMENTAL RESULTS
% -------------------------------------------------------------------------
\section{Experimental Results and Performance Comparison}
\label{sec:results_overview}

Building on the optimal features from Track 4B, seven machine learning algorithms representing different mathematical paradigms were evaluated: Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), Gradient Boosting Machine (GBM), XGBoost (XGB), and Support Vector Machines (SVM).

Figure \ref{fig:model_performance_chart} illustrates the performance of these algorithms across the three feature selection tracks. The blue bars represent the RFE-selected subset, which consistently outperformed other tracks across all model architectures.

\input{images/BARCHART}

\subsection{Performance Leaderboard}
Table \ref{tab:leaderboard} presents the final ranking of models on the Locked Test Set. Contrary to the initial hypothesis that complex ensemble models (XGBoost/GBM) would provide the best results, the leaderboard reveals a strong preference for linear and kernel-based classifiers.

\begin{table}[h!]
    \centering
    \caption{Final model leaderboard sorted by Test Accuracy, highlighting the trade-off between raw accuracy and the Macro F1-Score which accounts for class imbalance.}
    \label{tab:leaderboard}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l c c c} 
    \toprule
    \textbf{Rank} & \textbf{Model} & \textbf{Accuracy (\%)} & \textbf{F1-Score (Macro)} & \textbf{Overfit Gap} \\
    \midrule
    \textbf{1} & \textbf{SVM} & \textbf{89.49\%} & \textbf{0.610} & \textbf{0.12 (Excellent)} \\
    2 & Logistic Regression & 87.37\% & 0.617 & 0.18 (Good) \\
    3 & KNN & 84.17\% & 0.580 & 0.22 (Moderate) \\
    4 & XGBoost & 80.56\% & 0.559 & 0.25 (High) \\
    5 & Gradient Boosting & 80.38\% & 0.569 & 0.31 (Very High) \\
    6 & Random Forest & 78.85\% & 0.552 & 0.24 (High) \\
    7 & Decision Tree & 47.49\% & 0.447 & 0.22 (Moderate) \\
    \bottomrule
    \end{tabular}
\end{table}

% -------------------------------------------------------------------------
% SECTION 4.4: ALGORITHM ANALYSIS
% -------------------------------------------------------------------------
\section{Detailed Algorithm Analysis}
\label{sec:algorithm_analysis}

\subsection{The Success of Linearization}
The unexpected dominance of Support Vector Machines (89.49\% Accuracy) and Logistic Regression (0.617 F1-Score) can be attributed to the success of Stage 3.5 (Feature Expansion). By generating 8,700+ interaction terms, the pipeline essentially "linearized" a non-linear problem space. This allowed simple hyperplane-based models to separate complex defect classes effectively. The SVM, equipped with a Radial Basis Function (RBF) kernel, optimized class boundaries with high margin tolerance, making it robust to the inherent noise of real-world wafer maps.

\subsection{The Overfitting Trap of Ensemble Methods}
Gradient Boosting and XGBoost exhibited symptoms of the "Overfitting Trap." While these models achieved near-perfect F1-Scores (>0.90) on the balanced training data, their performance degraded significantly on the Test Set (yielding an Overfit Gap of 0.25 to 0.31). This indicates that the high variance of boosting algorithms caused them to "memorize" the synthetic data points generated by SMOTE. When faced with the organic, sparse distribution of the Locked Test Set, their complex decision boundaries failed to generalize, emphasizing the need for robust regularization in imbalanced industrial applications.

% -------------------------------------------------------------------------
% SECTION 4.5: ERROR ANALYSIS
% -------------------------------------------------------------------------
\section{Error Analysis and Misclassification Patterns}
\label{sec:error_analysis}

A granular analysis of the SVM Confusion Matrix (Figure \ref{fig:confusion_matrix}) reveals specific physical patterns where the model struggled. 

\subsection{Topological Confusion: Loc vs. Donut}
The most prominent error pattern involved the confusion between 'Loc' (Local) and 'Donut' defects. Physically, both appearing as concentrated clusters, they share similar Radon Transform signatures—specifically the aggregate pixel intensity along 1D projection lines. The primary difference is topological: a 'Donut' is hollow, whereas 'Loc' is a solid disc. Because the RFE process prioritized global Radon features to detect 'Scratches', it may have missed the subtle, localized standard deviation peaks required to identify the void in the center of small 'Donut' defects.

\subsection{Edge-Based Ambiguity}
A secondary error pattern was observed between 'Edge-Ring' and 'Edge-Loc'. The model occasionally misclassified localized clusters on the periphery as partial rings. This suggests that while radial features (distance from centroid) were highly predictive, the model lacked sufficient angular continuity features to distinguish between a discrete cluster (Edge-Loc) and an arc-shaped defect (Edge-Ring).

% -------------------------------------------------------------------------
% SECTION 4.6: GENERALIZABILITY AND DEPLOYMENT
% -------------------------------------------------------------------------
\section{Model Generalizability and Benchmarking}
\label{sec:generalizability}

The study utilized the "Overfit Gap" ($F1_{Train} - F1_{Test}$) as a KPI for industrial readiness. The SVM’s gap of 0.12 (Excellent) suggests high reliability for live deployment. In contrast, models with high gaps would likely generate excessive false alarms in a production line.

Table \ref{tab:study_comparison} benchmarks the proposed SVM approach against state-of-the-art literature utilizing the WM-811K dataset.

\begin{table}[h!]
    \centering
    \caption{Benchmarking results against previous studies. The proposed approach significantly outperforms traditional benchmarks and offers higher transparency than Deep Learning methods.}
    \label{tab:study_comparison}
    \begin{tabular}{l l c l} 
    \toprule
    \textbf{Study} & \textbf{Method} & \textbf{Accuracy (\%)} & \textbf{Feature Strategy} \\
    \midrule
    Wu et al. (2015) & RBF SVM & 82.5\% & Basic Geometric \\
    Adly et al. (2020) & Random Forest & 80.3\% & Basic Statistics \\
    Saqlain et al. (2019) & CNN (DL) & 96.2\% & Raw Pixels (GPU intensive) \\
    \textbf{Proposed Method} & \textbf{SVM (RFE-25)} & \textbf{89.5\%} & \textbf{Radon + Interaction} \\
    \bottomrule
    \end{tabular}
\end{table}

While Deep Learning approaches (CNN) achieve higher accuracy, they require significant computational power and offer zero interpretability. The proposed SVM pipeline achieves 89.5\% accuracy using only 25 selected features, allowing it to run on standard CPU-based manufacturing systems with near-zero latency, while providing engineers with clear evidence (Radon peaks) for every classification decision.

\section{Summary}
This chapter demonstrated that the combination of Recursive Feature Elimination and Support Vector Machines provides the optimal balance for semiconductor defect classification. The success of the 8,700+ feature expansion stage successfully simplified the problem for linear classifiers, resulting in a robust, interpretable, and production-ready solution that outperforms traditional benchmarks.
