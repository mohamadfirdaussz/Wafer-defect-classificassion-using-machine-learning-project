\chapter{RESULTS AND DISCUSSION}
\label{ch:results}

% -------------------------------------------------------------------------
% SECTION 4.1: OVERVIEW
% -------------------------------------------------------------------------
\section{Overview}
\label{sec:results_overview_intro}

This chapter details the empirical findings derived from the implementation of the automated machine learning pipeline methodology defined in Chapter 3. The experimental evaluation presented herein serves to validate the proposed leak-proof architectural design and systematically addresses the three specific study objectives outlined in Section \ref{sec:study objectives}

The structure of the analysis mirrors the sequential stages of the methodological framework to provide a coherent narrative. The evaluation commences in Section \ref{sec:feature_analysis}, which addresses the first research objective regarding the identification of an optimal feature set. This section evaluates the efficacy of the feature engineering strategies proposed in the methodology, specifically quantifying the impact of the Feature Combination stage. The analysis determines whether the reduction from a high-dimensional interaction space to a compact feature subset successfully optimizes model convergence while preserving discriminative information.

Fulfilling the second research objective, Section \ref{sec:results_overview} benchmarks the performance of the seven machine learning algorithms including Support Vector Machines (SVM), Random Forests, and Gradient Boosting configured during the Model Tuning phase. This comparative study identifies the optimal algorithmic paradigm for wafer defect classification, validating the selection of candidate models discussed in the experimental design and highlighting the trade-offs between computational complexity and classification accuracy.

Finally, Section \ref{sec:generalizability} addresses the third objective by assessing the robustness and generalizability of the trained models on the independent Test Set. This analysis directly validates the data partitioning strategy established in the Preprocessing stage, utilizing the Overfit Gap metric to confirm the system's reliability on unseen production data. Crucially, all results reported in this chapter are derived exclusively from this independent partition, ensuring that the findings reflect true generalization capability free from the data leakage bias often associated with synthetic oversampling techniques like SMOTE \parencite{Chawla_2002}.

Collectively, this chapter not only reports experimental performance but also interprets the observed behaviors in relation to defect morphology, feature representation, and model generalization. This dual emphasis ensures that the results are evaluated not only in terms of numerical performance, but also with respect to their physical plausibility and suitability for deployment in semiconductor manufacturing environments.
% -------------------------------------------------------------------------
% SECTION 4.2: FEATURE SELECTION
% -------------------------------------------------------------------------
\section{Feature Selection Analysis (Objective 1)}

\label{sec:feature_analysis}

As a prerequisite to meaningful model evaluation, this section addresses the first research objective: the identification of an optimal feature subset for wafer defect classification. Following the feature expansion stage, the dataset was transformed into a high-dimensional representation containing more than 8,700 interaction features derived from geometric, spatial, and transform-based descriptors. While this representation is highly expressive, its direct use in model training introduces substantial risks of overfitting, increased computational cost, and degraded generalization performance, a phenomenon commonly referred to as the Curse of Dimensionality \parencite{Keogh_2011}.

To mitigate these risks, feature selection was treated as a critical intermediate step that bridges feature engineering and classifier training. As emphasized by \textcite{Guyon_2003}, effective variable selection is essential not only for improving predictive performance but also for enhancing model interpretability and robustness. Accordingly, a systematic comparison of multiple feature selection strategies was conducted to identify an approach that preserves discriminative information while maintaining generalization stability.

To this end, the study evaluated multiple dimensionality reduction strategies organized into three experimental tracks. These tracks were explicitly designed to compare wrapper-based and embedded feature selection paradigms \parencite{Kohavi_1997}, thereby enabling an assessment of how different selection mechanisms influence downstream classification performance under identical data and preprocessing conditions.

\subsection{Description of Feature Selection Tracks and Subset Sizing}

This subsection describes the feature selection tracks evaluated in this study and explains the rationale behind the number of features retained in each track. Since feature selection directly affects model complexity, training stability, and classification performance, the determination of subset size was treated as an integral component of the experimental design. Different selection approaches impose different constraints on how features are ranked or eliminated; therefore, subset sizes were determined in a manner consistent with the underlying selection mechanism.

The investigation commenced with Track~4B, which employed Recursive Feature Elimination (RFE), a wrapper-based method that iteratively optimizes a base estimator by identifying and pruning the least significant descriptors based on model feedback \parencite{Guyon_2002}. The subset size for this track was empirically established at 25 using the elbow (knee detection) method \parencite{Satopaa_2011}. Preliminary tuning experiments revealed that classification performance saturated at this threshold; adding features beyond this point yielded diminishing improvements while increasing model variance. Consequently, a 25-feature subset was selected as the optimal trade-off between compactness and discriminative power.

To provide a rigorous counterpoint to the wrapper-based approach, Track~4C adopted an embedded feature selection strategy based on Random Forest importance. In this paradigm, feature relevance is quantified by the average reduction in Gini impurity contributed by each feature across the ensemble of decision trees \parencite{Breiman_2001}. To enable a controlled comparison with Track~4B, the feature count for Track~4C was deliberately constrained to the same 25-feature limit. By standardizing dimensionality across tracks, the experimental design isolates the selection mechanism itself as the independent variable, allowing a direct comparison between linear (RFE) and non-linear (Random Forest) ranking behaviors.

Finally, Track 4D implemented Lasso (Least Absolute Shrinkage and Selection Operator) regularization to explore an automatically derived feature subset. By applying an L1 penalty to the loss function, Lasso encourages sparsity by driving the coefficients of less informative features toward zero \parencite{Tibshirani_1996}. Unlike the fixed constraints imposed in Tracks 4B and 4C, the subset size in Track 4D was not manually specified. Instead, cross-validation was used to optimise the regularisation strength parameter (α), resulting in a subset of 77 features with non-zero coefficients. Minor variations in the selected features and coefficient values may be observed across different computing environments due to differences in numerical libraries, floating-point precision, parallel execution order, and cross-validation fold generation, which can affect optimisation trajectories in convex solvers. This outcome reflects the intrinsic sparsity structure identified by the regularisation process and allows evaluation of whether a larger, automatically determined subset offers improved robustness compared to the compact 25-feature configurations.

The selection of a 25-feature subset was motivated by both empirical performance trends and generalization considerations. Preliminary experiments showed that classification performance increased rapidly as features were added but plateaued once approximately 25 features were retained. Beyond this point, additional features produced negligible gains in Macro F1-score while consistently increasing training instability and variance. This behavior indicates that the most discriminative information was already captured within the top-ranked features, with subsequent descriptors introducing redundancy rather than complementary information.

From a generalization perspective, restricting the feature space to 25 descriptors also reduces the risk of overfitting, particularly in the presence of synthetic oversampling. Given the imbalanced nature of the dataset and the application of SMOTE during training, maintaining a compact feature representation is critical to preventing models from learning artificial patterns introduced by synthetic samples. Therefore, the 25-feature configuration represents an effective balance between representational sufficiency and robustness on unseen data.

\begin{table}[h!] \centering \small \caption{Comparison of the three feature selection tracks. The table explicitly distinguishes between the algorithmic mechanism and the rationale used to determine the final feature subset size.} \label{tab:feature_tracks_summary} \renewcommand{\arraystretch}{1.5} \begin{tabular}{l p{0.22\textwidth} p{0.55\textwidth} c} \toprule \textbf{Track} & \textbf{Method} & \textbf{Mechanism \& Sizing Logic} & \textbf{Count} \\ \midrule \textbf{4B} & \textbf{Wrapper} \newline Recursive Feature Elimination (RFE) & \textbf{Mechanism:} Iteratively trains the model and removes the weakest feature at each step until a target is reached. \newline \textbf{Sizing Logic:} \textit{Empirical Saturation.} Performance analysis showed model convergence at 25 features; adding more yielded diminishing returns. & \textbf{25} \\ \midrule \textbf{4C} & \textbf{Embedded} \newline Random Forest Importance & \textbf{Mechanism:} Ranks features based on their contribution to impurity reduction (Gini) across the ensemble trees. \newline \textbf{Sizing Logic:} \textit{Controlled Comparison.} The count was fixed to 25 specifically to match Track 4B, enabling a direct comparison between linear (RFE) and non-linear (RF) selection qualities. & \textbf{25} \\ \midrule \textbf{4D} & \textbf{Embedded} \newline Lasso Regularization & \textbf{Mechanism:} Applies an $L_1$ penalty to the loss function, mathematically forcing coefficients of irrelevant features to zero. \newline \textbf{Sizing Logic:} \textit{Automatic Discovery.} The count was not pre-selected. Optimizing the regularization parameter ($\alpha$) resulted in exactly 77 features having non-zero coefficients. & \textbf{77} \\ \bottomrule \end{tabular} \end{table}


For clarity, Table~\ref{tab:feature_tracks_summary} provides a consolidated overview of the three feature selection tracks and their corresponding sizing strategies. By jointly presenting the selection mechanism and subset determination logic, the table facilitates direct comparison across tracks and establishes a clear foundation for the comparative performance analysis presented in the subsequent subsection.
Lasso was the only method allowed to auto-select its feature count because sparsity is enforced directly by its objective, whereas RFE and Random Forest importance require an externally defined cutoff and therefore cannot determine an optimal subset size autonomously.
\clearpage

\subsection{Comparative Results}
\label{sec:Comparative Results}

The effectiveness of these selection strategies was validated by training a baseline classifier on each subset. Table \ref{tab:feature_tracks} summarizes the comparative performance. Given the imbalanced nature of the wafer defect dataset, the Macro F1-score was selected as the primary evaluation metric, as it equally weights performance across both majority and minority defect classes.

\begin{table}[h!]
    \centering
    \caption{Comparative analysis of feature selection strategies based on the Macro F1-Score. Track 4B (RFE) yielded the highest performance with the most compact feature set.}
    \label{tab:feature_tracks}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l c c} 
    \toprule
    \textbf{Track ID} & \textbf{Selection Method} & \textbf{Feature Count} & \textbf{Best Model Macro F1-Score} \\
    \midrule
    Track 4B & Wrapper (RFE) & 25 & 0.621 \\ 
    Track 4D & Embedded (Lasso) & 77 & 0.604 \\ 
    Track 4C & Embedded (Random Forest) & 25 & 0.595 \\ 
    \bottomrule
    \end{tabular}
\end{table}

Among the evaluated strategies, Track 4B based on Recursive Feature Elimination emerged as the most effective. By iteratively pruning weak features, RFE identified a compact subset of 25 features that enabled linear models to achieve superior predictive performance. In contrast, Track 4D produced a lower Macro F1-score despite retaining a larger number of features (77), suggesting that the $L_1$ penalty may have removed correlated yet complementary features essential for defining complex boundary conditions. Track 4C also underperformed relative to RFE, indicating that Gini-based importance was less effective at identifying the linear decision boundaries required for this specific dataset.

Based on these results, Track 4B was selected as the optimal feature selection strategy and was adopted for all subsequent model performance evaluations in Section \ref{sec:results_overview}.

These results indicate that classification performance is more strongly influenced by the quality and relevance of selected features than by their quantity. The superior performance of the compact RFE-derived subset suggests that wafer defect morphology can be effectively described using a small number of globally informative descriptors, while excessive dimensionality introduces redundancy that degrades generalization.

\begin{table}[h!]
\centering
\caption{Study-by-study feature comparison illustrating how features proposed in prior WM-811K-based studies correspond to, or are extended by, the overlapping and interaction-based features identified in this project.}
\label{tab:study_detailed_comparison}
\renewcommand{\arraystretch}{1.35}
\small
\begin{tabular}{p{0.25\textwidth} p{0.30\textwidth} p{0.35\textwidth}}
\toprule
\textbf{Reference Study} &
\textbf{Feature(s) Defined in Literature} &
\textbf{Corresponding / Enhanced Feature(s) in This Study} \\
\midrule

\multirow{3}{*}{\textbf{} \parencite{Wu_2014}}
& Radon transform statistics (mean, std dev) 
& \texttt{radon\_mean\_4}, \texttt{radon\_std\_9} incorporated within interaction features \\
& Geometric area 
& \texttt{radon\_* \_PLUS\_geom\_area} (Radon--Geometry interaction) \\
& Geometric perimeter 
& \texttt{radon\_* \_PLUS\_geom\_perimeter} (Radon--Geometry interaction) \\
\midrule

\multirow{3}{*}{\textbf{} \parencite{Piao_2018}} 
& Radon-based directional features 
& \texttt{radon\_mean\_11}, \texttt{radon\_mean\_12} (retained after feature selection) \\
& Density-based descriptors 
& \texttt{density\_9} (retained only under embedded selection, Track~4D) \\
& Feature utilisation strategy 
& Density features replaced by Radon--Density interaction terms in Track~4D \\
\midrule

\multirow{2}{*}{\textbf{} \parencite{Saqlain_2019}} 
& Basic geometric descriptors (major/minor axis) 
& \texttt{geom\_major\_axis}, \texttt{geom\_minor\_axis} used within composite features \\
& Deep learned representations 
& \texttt{geom\_solidity\_PLUS\_stat\_median} as an interpretable, hand-crafted alternative \\
\bottomrule
\end{tabular}
\end{table}

Figure \ref{fig:model_performance_chart} presents a comparative overview of model performance across the three feature selection tracks. The visualization highlights a consistent performance advantage for Track 4B (RFE) across most model architectures, with particularly strong gains observed for linear and kernel-based classifiers.

\begin{figure}[h!]
    \centering
    % Adjust 1.1 up to 1.2 or 1.3 if you have wide margins
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{images/model_performance_chart.png}}
    \caption{Comparative performance of seven machine learning models across three feature selection tracks. Track 4B (RFE) consistently achieves higher Macro F1-scores, particularly for Support Vector Machines and Logistic Regression.}
    \label{fig:model_performance_chart}
\end{figure}



% -------------------------------------------------------------------------
% SECTION 4.3: EXPERIMENTAL RESULTS
% -------------------------------------------------------------------------
\newpage
\section{Performance of Traditional Machine Learning Algorithms (Objective 2)}
\label{sec:results_overview}

Having established an effective feature selection strategy (Track 4B) as discussed in Section\ref{sec:Comparative Results}, the analysis now shifts from feature-level optimization to model-level performance evaluation. This section examines how different traditional machine learning algorithms respond to the selected feature representations and whether variations in algorithmic complexity translate into improved predictive performance on unseen wafer data. By evaluating a diverse set of classifiers under a unified experimental framework, this section aims to determine not only which model achieves the highest accuracy but also which offers the most balanced and generalizable performance in the presence of class imbalance.

By fixing the feature representation to the optimal Track 4B subset, this analysis isolates algorithmic behavior as the sole experimental variable. This controlled design enables a direct assessment of how model complexity and inductive bias influence classification performance under identical data and feature conditions.

% Figure \ref{fig:model_performance_chart} presents a comparative overview of model performance across the three feature selection tracks. The visualization highlights a consistent performance advantage for Track 4B (RFE) across most model architectures, with particularly strong gains observed for linear and kernel-based classifiers.

% \begin{figure}[h!]
%     \centering
%     % Adjust 1.1 up to 1.2 or 1.3 if you have wide margins
%     \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{images/model_performance_chart.png}}
%     \caption{Comparative performance of seven machine learning models across three feature selection tracks. Track 4B (RFE) consistently achieves higher Macro F1-scores, particularly for Support Vector Machines and Logistic Regression.}
%     \label{fig:model_performance_chart}
% \end{figure}

\clearpage
\subsection{Performance Leaderboard}
To synthesize the comparative results, Table \ref{tab:leaderboard} consolidates the performance metrics into a final leaderboard based exclusively on the locked test set using the optimal feature subset (Track 4B). The ranking considers both overall classification accuracy and the Macro F1-score, thereby explicitly highlighting the trade-off between raw predictive correctness and equitable performance across minority and majority defect classes. This consolidated perspective provides a clearer basis for assessing practical model suitability in manufacturing environments, where generalization stability and class-wise reliability are critical deployment considerations.

Accuracy is reported to quantify overall classification correctness, while the Macro F1-score is used to evaluate balanced performance across both majority and minority defect classes. In addition, the Overfit Gap is included as an indicator of generalization stability, capturing the divergence between training and test performance under identical feature and preprocessing conditions.
\begin{table}[h!]
    \centering
    \caption{Final model rankings using Track 4B features, sorted by Test Accuracy to highlight the trade-off between Accuracy and Macro F1-Score.}
    \label{tab:leaderboard}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l c c c} 
    \toprule
    \textbf{Rank} & \textbf{Model} & \textbf{Accuracy (\%)} & \textbf{F1-Score (Macro)} & \textbf{Overfit Gap} \\
    \midrule
    1 & SVM & 88.42\% & 0.606 & 0.12 (Excellent) \\
    2 & Logistic Regression & 82.81\% & 0.621 & 0.20 (Good) \\
    3 & KNN & 81.65\% & 0.553 & 0.24 (Moderate) \\
    4 & Random Forest & 79.16\% & 0.545 & 0.26 (High) \\
    5 & XGBoost & 78.79\% & 0.545 & 0.27 (High) \\
    6 & Gradient Boosting & 78.50\% & 0.557 & 0.32 (Very High) \\
    7 & Decision Tree & 50.90\% & 0.457 & 0.21 (Moderate) \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Performance of Linear Models (SVM and Logistic Regression)}
To further visualize the specific classification behaviors of the top-performing models, Figure \ref{fig:confusion_matrix_svm} displays the Confusion Matrix for the Support Vector Machine (SVM), while Figure \ref{fig:confusion_matrix_lr} presents the Confusion Matrix for Logistic Regression.

The first focus of the algorithm-level analysis is the consistently strong performance of linear classifiers, particularly Support Vector Machines and Logistic Regression. As observed previously in Figure \ref{fig:model_performance_chart} and Table \ref{tab:leaderboard}, their dominance across both accuracy-based and class-balanced metrics indicates that the transformed feature space enables effective separation of defect classes using relatively simple decision boundaries. 

\begin{figure}[h!]
    \centering
    % Ensure 'confusion_matrix.png' is in your images folder
    \includegraphics[width=1.2\linewidth]{images/svm cm.png} 
    \caption{Confusion Matrix generated by the SVM model (Track 4B) on the unseen Test Set. The distinct diagonal line indicates high classification accuracy across the majority of defect classes.}
    \label{fig:confusion_matrix_svm}
\end{figure}

The SVM emerged as the top Accuracy (88.42\%) by effectively maximizing the margin between defect classes. The use of the Radial Basis Function (RBF) kernel likely allowed the model to handle slight non-linearities in the boundary while maintaining robustness against noise \parencite{Chang_2011}.

Simultaneously, Logistic Regression proved to be the Best Balance, achieving the highest Macro F1-Score (0.621). This demonstrates its superior capability in detecting minority classes. This success suggests that the extensive Feature Expansion  Stage  successfully "linearized" the problem space, allowing simpler decision boundaries to separate complex defects like 'Loc' and 'Edge-Ring'.

As shown in Figure \ref{fig:confusion_matrix_lr}, the Logistic Regression model demonstrates a strong ability to correctly classify minority defects, as evidenced by the relatively high counts along the diagonal for classes like 'Center' (773), 'Edge-Ring' (1838), and 'Loc' (424).Its high overall recall of 0.805 indicates that it successfully captures a large proportion of true defects.
However, this sensitivity comes with a trade-off, as seen in the significant number of 'none' class samples misclassified as 'Loc' (1481) and 'Scratch' (1820). This pattern of "over-flagging" is characteristic of a model optimized for recall on imbalanced data, where missing a true defect is considered more costly than a false alarm.

\begin{figure}[h!]
    \centering
    % Ensure 'confusion_matrix_lr.png' is in your images folder
    \includegraphics[width=1.2\linewidth]{images/logisresg.png}
    \caption{Confusion Matrix generated by the Logistic Regression model (Track 4B) on the unseen Test Set. The model achieves a high recall of 0.80, demonstrating its effectiveness at capturing true defects, albeit with a higher rate of false positives from the 'none' class compared to SVM.}
    \label{fig:confusion_matrix_lr}
\end{figure}

\newpage
\subsection{Confusion Between Morphologically Similar Classes}
The most significant source of classification error occurred between the \textbf{'Loc' (Local)} and \textbf{'Donut'} defect classes. Approximately 12\% of 'Loc' defects were misclassified as 'Donut'. This confusion is physically explainable, as both defect types appear as clustered, circular geometries on the wafer map. 

The primary difference is topological: 'Loc' is a solid "blob," whereas 'Donut' is a hollow ring. The feature selection process (Track 4B) prioritized Radon Transform features, which measure aggregate pixel intensity along projection lines. For small or thick-walled Donut defects, the Radon projection profile is mathematically very similar to that of a solid Loc defect, causing the SVM hyperplane to lack the resolution required to distinguish the "hole" in the center.

\subsection{Edge-Based Ambiguity}
A secondary error pattern was observed between \textbf{'Edge-Ring'} and \textbf{'Edge-Loc'}. These defects are spatially identical (located at the wafer periphery) but differ in distribution shape. The model occasionally failed to distinguish a partial ring (Edge-Ring) from a localized cluster near the edge (Edge-Loc). This suggests that while the current geometric features (e.g., Eccentricity, Centroid Distance) successfully captured the radial position of the defect, they were less effective at characterizing the angular continuity of the defect along the wafer circumference.



\subsection{Performance of Ensemble Methods}
In contrast to the stability of linear models, ensemble-based architectures (Gradient Boosting and XGBoost) exhibited a marked discrepancy between training and testing performance. Despite their high theoretical representational capacity, these models demonstrated a susceptibility to overfitting in this specific domain.

Quantitative analysis reveals a substantial generalization gap  while both Gradient Boosting and XGBoost achieved near-perfect performance on the training set, their predictive capability degraded significantly on the independent Test Set. This divergence suggests that the high variance inherent in boosting algorithms led to the "memorization" of the training data distribution.

Crucially, this behavior implies a detrimental interaction between the boosting mechanism and the SMOTE data augmentation technique. Because boosting algorithms iteratively focus on hard-to-classify instances, they likely over-optimized for the synthetic samples generated by SMOTE.  Consequently, the models constructed overly complex, jagged decision boundaries that fit the synthetic noise rather than the underlying defect morphology. When applied to the "organic" noise patterns of the unseen Locked Test Set, these brittle boundaries failed to generalize, highlighting the risks of applying high-complexity models to noisy manufacturing data without aggressive regularization.

It should be noted that the underperformance of ensemble methods in this study does not imply their general unsuitability for wafer defect classification. Rather, it highlights their sensitivity to high-dimensional noise and synthetic oversampling under a strict leak-proof evaluation protocol, emphasizing the importance of aligning model complexity with data characteristics.


In direct response to Objective 2, the results demonstrate that linear and kernel-based classifiers outperform ensemble methods when paired with compact, RFE-optimized feature representations. Support Vector Machines achieved the highest overall accuracy, while Logistic Regression provided the most balanced performance across minority defect classes. These findings confirm that increased algorithmic complexity does not necessarily yield superior performance in highly engineered, imbalanced wafer datasets, and that generalization stability is better achieved through disciplined feature selection and regularization.


% -------------------------------------------------------------------------
% SECTION 4.6: GENERALIZABILITY
% -------------------------------------------------------------------------
\newpage
\section{Model Generalizability (Objective 3)}
\label{sec:generalizability}

Having identified the optimal feature representation (Objective 1) \ref{sec:feature_analysis} and benchmarked algorithmic performance under a controlled experimental setting (Objective 2)\ref{sec:results_overview}, the final objective of this study focuses on evaluating model generalizability. Specifically, Objective 3 seeks to determine whether the observed classification performance can be reliably sustained when models are exposed to previously unseen, real-world wafer data—an essential requirement for practical deployment in semiconductor manufacturing environments.

While Section~\ref{sec:results_overview} established that linear and kernel-based classifiers outperform more complex ensemble methods in terms of balanced classification metrics, high predictive performance alone is insufficient for industrial adoption. A deployed system must also demonstrate stability under distributional shift and must avoid performance collapse when encountering new production conditions. This concern is particularly relevant in the present study, as Synthetic Minority Over-sampling Technique (SMOTE) was applied during training to mitigate severe class imbalance.

Although SMOTE improves minority-class learning, it introduces a well-documented risk: models may inadvertently learn synthetic artifacts rather than physically meaningful defect patterns. This risk is especially pronounced in high-capacity learners such as boosting-based ensembles, which are prone to memorizing fine-grained variations in the training distribution. Therefore, an explicit and quantitative assessment of overfitting behavior is required to validate whether the models identified in Objective 2 are genuinely deployment-ready.

To address this, model generalizability was evaluated by comparing performance on the SMOTE-augmented training set against performance on a large, independent real-world test set comprising 34,560 samples. The divergence between these two phases was quantified using the \textit{Overfit Gap}, defined as the numerical difference between the Training F1-score and the Test F1-score. A smaller gap indicates stronger generalization capability and lower operational risk.

The results of this analysis are summarized in Table~\ref{tab:overfit_gap}.

\begin{table}[h!]
    \centering
    \caption{Generalizability analysis quantified using the Overfit Gap. A lower gap indicates a more robust and deployment-ready model.}
    \label{tab:overfit_gap}
    \begin{tabular}{l c c c} 
    \toprule
    \textbf{Model} & \textbf{Train F1 (SMOTE)} & \textbf{Test F1 (Real)} & \textbf{Gap} \\
    \midrule
    SVM & 0.72 & 0.61 & 0.11 (Best) \\
    Logistic Regression & 0.79 & 0.62 & 0.17 \\
    XGBoost & 0.84 & 0.56 & 0.28 \\
    Gradient Boosting & 0.90 & 0.57 & 0.33 (Worst) \\
    \bottomrule
    \end{tabular}
\end{table}

Among all evaluated models, the Support Vector Machine (SVM) exhibited the smallest Overfit Gap of 0.123, indicating highly stable behavior between training and testing phases. This finding reinforces the conclusions drawn in Objective 2, where SVM demonstrated strong accuracy with consistent class-wise performance. The observed stability is consistent with the structural risk minimization principle underlying SVMs, which favors simpler decision boundaries and inherently limits overfitting. Despite not achieving the highest training F1-score, the SVM maintained reliable performance on unseen data, making it the most robust candidate for real-world deployment.

Logistic Regression demonstrated moderate generalizability with an Overfit Gap of 0.195. While its linear formulation limits representational capacity, this constraint simultaneously acts as a form of regularization, preventing excessive memorization of synthetic samples. The model’s strong recall performance across multiple feature tracks suggests that it remains resilient under domain shift, albeit with a higher false-positive rate compared to SVM.

In contrast, ensemble-based methods such as XGBoost and Gradient Boosting exhibited substantially larger Overfit Gaps of 0.269  and 0.321, respectively. Although these models achieved very high training F1-scores, their performance degraded significantly on the independent test set. This behavior confirms the qualitative observations reported in Section~\ref{sec:results_overview}: the boosting mechanisms over-adapted to SMOTE-generated samples, resulting in overly complex decision boundaries that failed to generalize to real production data.

From an industrial perspective, such instability represents a critical deployment risk. A model that performs well only under synthetic training conditions may fail unpredictably when exposed to evolving defect distributions, process drift, or new manufacturing regimes.

In direct fulfillment of Objective 3, this analysis demonstrates that generalization stability not peak training performance is the defining criterion for model selection in imbalanced wafer defect classification. Models with constrained complexity and consistent train–test behavior, particularly Support Vector Machines, provide the most reliable balance between accuracy, robustness, and operational safety. These findings validate the leak-proof evaluation framework proposed in this study and confirm its effectiveness in identifying models suitable for deployment in real semiconductor fabrication environments.

.

% -------------------------------------------------------------------------
% SECTION 4.7: COMPARISON
% -------------------------------------------------------------------------
\section{Comparison with Previous Studies}
\label{sec:comparison}

To validate the effectiveness of the proposed Radon-based feature engineering approach, it is essential to benchmark the obtained results against existing literature. This section compares the performance of the optimized SVM pipeline with other methodologies applied to the same WM-811K dataset. The comparison focuses not only on raw classification accuracy but also on the trade-offs between computational efficiency, model interpretability, and resource requirements.

Table \ref{tab:study_comparison} contextualizes the contribution of this work against selected state-of-the-art studies.
\begin{table}[h!]
    \centering
    \caption{Benchmarking the proposed method against state-of-the-art literature.}
    \label{tab:study_comparison}
    \small % 1. Slightly smaller font helps fit more text
    \renewcommand{\arraystretch}{1.3} % Adds breathing room between rows
    
    % 2. tabularx set to \textwidth ensures it never goes off the page
    % 3. The 'X' column type automatically wraps long text
    \begin{tabularx}{\textwidth}{l l c X} 
    \toprule
    \textbf{Study} & \textbf{Method} & \textbf{Acc.} & \textbf{Feature Approach} \\
    \midrule
    Wu et al. (2015) \parencite{Wu_2014} & RBF SVM & 82.5\% & Geometric Features \\
    Adly et al. (2020) \parencite{Adly_2020} & Rand. Forest & 80.3\% & Basic Statistics \\
    Wang et al. (2019) \parencite{Wang_2019} & Log. Reg. & 68.4\% & Baseline Pixel Counts \\
    Saqlain et al. (2019) \parencite{Saqlain_2019} & CNN (Deep) & 96.2\% & Raw Pixel Data \\
    \midrule
    \textbf{Proposed Method} & \textbf{SVM (RFE)} & \textbf{88.42\%} & \textbf{Radon + Interaction} \\
    \bottomrule
    \end{tabularx}
\end{table}

The proposed method achieves 88.42\% accuracy, significantly outperforming historical traditional machine learning benchmarks which typically ranged between 68\% and 82.5\%. While Deep Learning approaches like CNNs \parencite{Saqlain_2019} achieve higher raw accuracy (96\%), they require massive GPU resources and lack interpretability. The proposed SVM pipeline offers a strategic advantage in efficiency and explainability. By achieving nearly 90\% accuracy using only 25 features and a standard SVM, the solution can be deployed on legacy manufacturing hardware (CPUs) with minimal latency, while still allowing engineers to understand exactly why a defect was classified based on specific Radon peaks.

% -------------------------------------------------------------------------
% SECTION 4.8: LIMITATIONS & SUMMARY
% -------------------------------------------------------------------------
\section{Limitations}
\label{sec:limitations}

The experimental results presented in this chapter confirm the efficacy of the proposed pipeline; however, the scope of this research remains bounded by specific constraints. Acknowledging these limitations is vital for defining the boundaries of validity for the current findings and for identifying fruitful directions for future work. 

First, the evaluation was restricted to the WM-811K dataset; validating on data from different semiconductor foundries would further confirm robustness. Second, while SMOTE improved minority class detection, the F1-scores for the rarest classes remain lower than those for majority classes, indicating that extreme imbalance persists as a challenge. Finally, the study prioritized interpretability over raw accuracy; a hybrid approach combining CNN feature extraction with SVM classification could potentially bridge the performance gap to deep learning approaches.


\section{Chapter Summary}

This chapter presented the experimental results of the proposed leak-proof machine learning framework for wafer defect classification, addressing all three research objectives. First, feature selection analysis showed that Recursive Feature Elimination (Track~4B) provides a compact and informative feature set, reducing dimensionality to 25 descriptors while preserving discriminative defect information. This confirms that wafer defect morphology can be effectively represented using a small number of carefully engineered geometric and Radon-based features.

Using this optimized feature set, multiple machine learning models were evaluated and compared. Linear and kernel-based models, particularly Support Vector Machines, consistently outperformed more complex ensemble methods. Generalizability analysis using the Overfit Gap metric further demonstrated that SVM achieved the most stable performance on unseen data, making it the most suitable model for industrial deployment. Overall, the results show that reliable wafer defect classification is achieved through disciplined feature engineering and controlled model complexity rather than highly complex algorithms, providing a strong foundation for the conclusions presented in Chapter~5.

