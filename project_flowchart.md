
-----

# ğŸ“‚ Project Structure

This directory tree reflects the **5-Stage Pipeline** architecture designed to prevent data leakage and handle high-dimensional feature selection.

```plaintext
wm811k_wafer_project/
â”‚
â”œâ”€â”€ ğŸ“ datasets/                        # ğŸ›‘ INPUT DATA
â”‚   â””â”€â”€ LSWMD.pkl                       # Raw WM-811K dataset (Pickle format)
â”‚
â”œâ”€â”€ ğŸ“ data_loader_results/             # âŠ Stage 1 Output
â”‚   â””â”€â”€ cleaned_full_wm811k.npz         # Cleaned, resized (64x64) wafer maps
â”‚
â”œâ”€â”€ ğŸ“ Feature_engineering_results/     # â‹ Stage 2 Output
â”‚   â””â”€â”€ features_dataset.csv            # 66 Extracted Features (Radon, Density, Geometry)
â”‚
â”œâ”€â”€ ğŸ“ preprocessing_results/           # âŒ Stage 3 & 3.5 Output
â”‚   â”œâ”€â”€ standard_scaler.joblib          # Saved Scaler object
â”‚   â”œâ”€â”€ model_ready_data.npz            # Balanced Training & Locked Test Sets
â”‚   â””â”€â”€ data_track_4E_Full_Expansion_expanded.npz  # 8,500+ Expanded Features
â”‚
â”œâ”€â”€ ğŸ“ feature_selection_results/       # â Stage 4 Output (The "Tracks")
â”‚   â”œâ”€â”€ data_track_4B_RFE.npz           # Track B: Recursive Feature Elimination
â”‚   â”œâ”€â”€ data_track_4C_RF_Importance.npz # Track C: Random Forest Importance
â”‚   â”œâ”€â”€ data_track_4D_Lasso.npz         # Track D: Lasso (L1) Selection
â”‚   â””â”€â”€ RF_Feature_Importance_Ranking.csv
â”‚
â”œâ”€â”€ ğŸ“ model_artifacts/                 # â Stage 5 Output (Final Results)
â”‚   â”œâ”€â”€ master_model_comparison.csv     # ğŸ† Final Leaderboard
â”‚   â”œâ”€â”€ 4D_Lasso/
â”‚   â”‚   â”œâ”€â”€ LogisticRegression/
â”‚   â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”‚   â”‚   â””â”€â”€ classification_report.txt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“œ main.py                          # ğŸš€ MASTER CONTROLLER (Run this)
â”œâ”€â”€ ğŸ“œ test_pipeline.py                 # Unit Test Suite
â”‚
â”œâ”€â”€ ğŸ“œ data_loader.py                   # Stage 1: Cleaning & Resizing
â”œâ”€â”€ ğŸ“œ feature_engineering.py           # Stage 2: Feature Extraction
â”œâ”€â”€ ğŸ“œ data_preprocessor.py             # Stage 3: Split, Scale, Hybrid Balance
â”œâ”€â”€ ğŸ“œ feature_combination.py           # Stage 3.5: High-Dimensional Expansion
â”œâ”€â”€ ğŸ“œ feature_selection.py             # Stage 4: Selection Funnel
â”œâ”€â”€ ğŸ“œ model_tuning.py                  # Stage 5: Training & Evaluation
â””â”€â”€ ğŸ“œ model_tuning_optimized.py        # Stage 6: Optimized Tuning (Anti-Overfitting)
```

-----

# ğŸ“ˆ Pipeline Flowchart

This workflow is strictly sequential to ensure **Zero Data Leakage**. The Test Set is locked away in Stage 3 and never touched until Stage 5.

```plaintext
                                â–¶ WAFER DEFECT CLASSIFICATION PIPELINE â—€
                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      1ï¸âƒ£  DATA INGESTION & CLEANING
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Load 'LSWMD.pkl' (800k+ wafers)
      â€¢ Filter: Remove 'Near-full' and unlabeled wafers
      â€¢ Denoise: 2x2 Median Filter
      â€¢ Resize: Nearest Neighbor Interpolation â†’ 64x64
      â€¢ Output: Cleaned 3D Numpy Array
                                       â”‚
                                       â–¼
      2ï¸âƒ£  FEATURE ENGINEERING (The "Descriptors")
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Density Features (13 regions)
      â€¢ Radon Transform (40 angles) â†’ Detects lines/scratches
      â€¢ Geometry (Area, Perimeter, Solidity, Eccentricity)
      â€¢ Statistics (Mean, Skew, Kurtosis)
                                       â”‚
                                       â–¼
      3ï¸âƒ£  PREPROCESSING (The "Gatekeeper")
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ ğŸ›¡ï¸ CRITICAL: Stratified Split (70% Train / 30% Test)
      â€¢ Scaling: StandardScaler (Fit on Train â†’ Transform Test)
      â€¢ âš–ï¸ HYBRID BALANCING (Train Set ONLY):
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Major Class ('none')   â†’  ğŸ“‰ Undersample to 2,500       â”‚
          â”‚  Minor Classes ('Donut') â†’  ğŸ“ˆ SMOTE Upsample to 500     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
      3ï¸âƒ£.5ï¸âƒ£  FEATURE EXPANSION
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Interaction Terms: A Ã— B, A Ã· B, A + B
      â€¢ Result: Explosion from 66 â†’ ~8,500 Features
                                       â”‚
                                       â–¼
      4ï¸âƒ£  FEATURE SELECTION FUNNEL
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ âš¡ Step 1: ANOVA Pre-filter (Discard 8,500 â†’ 1,000 features)
      â€¢ ğŸ” Step 2: Fine Selection Tracks
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   TRACK 4B    â”‚      TRACK 4C       â”‚      TRACK 4D       â”‚
          â”‚  Wrapper RFE  â”‚    Embedded RF      â”‚    Embedded Lasso   â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                   â”‚                     â”‚
                 â–¼                   â–¼                     â–¼
      5ï¸âƒ£  MODEL TUNING & BAKE-OFF
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Algorithms: LR, KNN, SVM, DT, RF, GBM, XGBoost
      â€¢ Method: GridSearchCV + 3-Fold Stratified Cross-Validation
      â€¢ Constraint: Strict Regularization (Straitjacket) to prevent Overfitting
                                       â”‚
                                       â–¼
      6ï¸âƒ£  FINAL EVALUATION
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Input: The Locked Test Set (Imbalanced/Organic)
      â€¢ Metrics: F1-Macro, "Overfit Gap" (Train vs Test)
      â€¢ Visuals: Confusion Matrix, ROC Curve, Feature Importance
                                       â”‚
                                       â–¼
      6ï¸âƒ£  OPTIMIZED TUNING ("Nuclear Option")
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â€¢ Trigger: If Overfit Gap > 0.10
      â€¢ Tactic 1: Data Pruning (Drop 10% Train Samples)
      â€¢ Tactic 2: Gaussian Jitter (Blur Synthetic Points)
      â€¢ Result: Robust "Generalist" Model saved to preprocessing_results/
```

-----

# ğŸ§ª Experiment Tracks (Recipes)

The pipeline automatically runs these competing strategies in **Stage 4** and **Stage 5**.

### ğŸ… Track 4B: Recursive Feature Elimination (RFE)

  * **Logic:** Greedily removes the weakest feature, retrains, and repeats.
  * **Best For:** Finding a compact subset of features that work well *together*.
  * **Configuration:** Logistic Regression Estimator, Target = 25 features.

### ğŸ…‚ Track 4C: Random Forest Importance

  * **Logic:** Measures how much "Gini Impurity" decreases when a feature is used to split a node.Gini impurity is a metric used in decision trees to measure node impurity, which is the likelihood of incorrectly classifying a randomly chosen element if it were labeled randomly. A low Gini impurity score indicates a node is pure (e.g., all elements belong to one class), while a high score means it is impure and mixed with many different classes
  * **Best For:** Finding non-linear relationships.
  * **Configuration:** 100 Trees, Target = 25 features.

### ğŸ…ƒ Track 4D: Lasso (L1) Regularization 

  * **Logic:** Applies a penalty ($\lambda$) to the coefficients. Weak features are mathematically forced to exactly zero.
  * **Best For:** High-dimensional sparse data (like our 8,500 features).
  * **Configuration:** `C=0.01` (Strong Penalty).

-----

# ğŸƒ How to Run

### âœ… One-Click Execution

Run the master controller. This checks for dependencies, creates folders, and runs all 5 stages in order.

```bash
python main.py
```

### ğŸ› ï¸ Manual Step-by-Step

If you need to debug a specific stage:

**1. Clean Data**

```bash
python data_loader.py
```

**2. Extract Features**

```bash
python feature_engineering.py
```

**3. Split & Balance (Critical Step)**

```bash
python data_preprocessor.py
```

**4. Expand & Select Features**

```bash
python feature_combination.py
python feature_selection.py
```

**5. Train & Evaluate**

```bash
python model_tuning.py
```

**6. Optimized Tuning**

```bash
python model_tuning_optimized.py
```

### ğŸ§ª Run Unit Tests

Before submitting your thesis code, verify integrity:

```bash
python test_pipeline.py
```

*Expected Output:* `Ran 5 tests in 0.4s ... OK`